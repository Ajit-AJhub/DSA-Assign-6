{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddac023c-c9b9-4a9a-bbfa-1937ad36d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Data Ingestion Pipeline:\n",
    "   a. Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms.\n",
    "   b. Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\n",
    "   c. Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2181462-c300-4246-b057-d57c5d9274bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a. Designing a Data Ingestion Pipeline for Collecting and Storing Data from Various Sources:\n",
    "\n",
    "To design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms, \n",
    "we can follow these steps:\n",
    "\n",
    "1. Identify the data sources: Determine the different types of data sources you want to collect data from, such as relational databases,\n",
    "NoSQL databases, web APIs, message queues, and streaming platforms.\n",
    "\n",
    "2. Define the data ingestion process: Determine how the data will be retrieved from each source. This may involve writing custom code or\n",
    "using existing tools and connectors provided by the data sources.\n",
    "\n",
    "3. Extract data from databases: For relational databases, you can use SQL queries to extract the required data. Tools like Apache Sqoop or\n",
    "custom scripts can be used to extract data from NoSQL databases.\n",
    "\n",
    "4. Access data from APIs: Use API documentation provided by the data source to understand how to authenticate and retrieve data using API calls. \n",
    "Use libraries or frameworks like requests (Python), RestSharp (.NET), or Retrofit (Java) to interact with the APIs.\n",
    "\n",
    "5. Connect to streaming platforms: If you need to ingest real-time data from streaming platforms like Apache Kafka, Apache Flink, or Apache \n",
    "Spark, use their respective client libraries to establish a connection and consume the data.\n",
    "\n",
    "6. Transform and cleanse data: Perform any necessary transformations and cleansing steps on the incoming data to ensure its quality\n",
    "and compatibility with your data storage format. This may include data type conversions, removing duplicates, or handling missing values.\n",
    "\n",
    "7. Store data: Determine the storage mechanism for your data. You can use various options such as relational databases, data lakes \n",
    "(Hadoop Distributed File System - HDFS), cloud storage (Amazon S3, Google Cloud Storage), or specialized databases like Apache Cassandra or MongoDB.\n",
    "\n",
    "8. Choose a pipeline orchestration tool: To automate and manage the data ingestion pipeline, consider using a workflow orchestration tool\n",
    "like Apache Airflow, Apache NiFi, or AWS Step Functions. These tools allow you to schedule and monitor data ingestion tasks, handle dependencies,\n",
    "and manage retries.\n",
    "\n",
    "9. Monitor and error handling: Implement logging and monitoring mechanisms to track the pipeline's health and identify any failures. \n",
    "Set up alerting systems to notify you when issues occur. Implement error handling strategies such as retry mechanisms, dead-letter queues, or\n",
    "automated error notifications.\n",
    "\n",
    "10. Data governance and security: Ensure data governance and security practices are in place. Consider data encryption, access controls, \n",
    "and compliance requirements to protect sensitive data.\n",
    "\n",
    "b. Implementing a Real-time Data Ingestion Pipeline for IoT Sensor Data:\n",
    "\n",
    "To implement a real-time data ingestion pipeline for processing sensor data from IoT devices, you can follow these steps:\n",
    "\n",
    "1. Data collection from IoT devices: Set up IoT devices to collect and send sensor data in real-time. Each device should be uniquely identified,\n",
    "and the data should be formatted appropriately for transmission.\n",
    "\n",
    "2. IoT message broker: Use a message broker such as Apache Kafka or RabbitMQ to handle real-time data streams from the IoT devices. \n",
    "Configure topics or queues to receive and distribute the incoming sensor data.\n",
    "\n",
    "3. Data ingestion layer: Develop a data ingestion layer that connects to the message broker and consumes the sensor data. Use client libraries or \n",
    "frameworks provided by the message broker to establish a connection and consume the messages.\n",
    "\n",
    "4. Real-time data processing: Apply real-time processing techniques such as stream processing frameworks \n",
    "(Apache Kafka Streams, Apache Flink, Apache Spark Streaming) or complex event processing (CEP) engines to analyze and transform the \n",
    "incoming sensor data.\n",
    "\n",
    "5. Data storage: Decide on the appropriate storage mechanism based on your requirements. This can include data lakes, time-series databases \n",
    "(InfluxDB, TimescaleDB), or NoSQL databases (Apache Cassandra, MongoDB).\n",
    "\n",
    "6. Data visualization and analysis: Set up tools or dashboards to visualize and analyze the processed data. Use libraries like Matplotlib,\n",
    "Plotly, or specialized data visualization tools like Tableau or Grafana to create meaningful visualizations.\n",
    "\n",
    "7. Real-time alerts and notifications: Implement alerting mechanisms to trigger notifications or actions based on specific conditions or \n",
    "thresholds defined in the sensor data. This can involve sending notifications via email, SMS, or triggering external services.\n",
    "\n",
    "8. Scalability and fault tolerance: Design the pipeline to handle scalability and fault tolerance. Consider horizontal scaling of the processing \n",
    "components, data partitioning strategies, and replication mechanisms to ensure high availability.\n",
    "\n",
    "c. Developing a Data Ingestion Pipeline for Handling Different File Formats and Performing Data Validation and Cleansing:\n",
    "\n",
    "To develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and\n",
    "cleansing, you can follow these steps:\n",
    "\n",
    "1. File ingestion: Implement file ingestion mechanisms to read data from various file formats such as CSV, JSON, XML, or Parquet. \n",
    "Use appropriate libraries or frameworks in your programming language of choice to parse the files and extract the data.\n",
    "\n",
    "2. Data validation: Define validation rules and checks to ensure the integrity and quality of the ingested data. This can include verifying data\n",
    "types, validating against predefined schemas, checking for missing or duplicate values, or applying business rules.\n",
    "\n",
    "3. Data cleansing: Implement data cleansing steps to improve data quality and consistency. This may involve removing or handling missing values,\n",
    "standardizing formats, correcting inconsistent data, or deduplicating records.\n",
    "\n",
    "4. Transformation: Perform any necessary data transformations to align the ingested data with your target data model or storage format. \n",
    "This can include renaming columns, aggregating data, splitting or merging data fields, or applying data type conversions.\n",
    "\n",
    "5. Data storage: Determine the appropriate data storage mechanism based on your requirements. This can include relational databases,\n",
    "data lakes, NoSQL databases, or cloud storage solutions.\n",
    "\n",
    "6. Error handling and logging: Implement error handling mechanisms to capture and handle any issues that arise during data ingestion, \n",
    "validation, or cleansing. Log error messages and consider implementing retries or error notification mechanisms.\n",
    "\n",
    "7. Automation and scheduling: If the data ingestion pipeline is expected to run regularly or at specific intervals, consider using a \n",
    "scheduling tool or framework like Apache Airflow, cron jobs, or Azure Data Factory to automate the ingestion process.\n",
    "\n",
    "8. Monitoring and reporting: Set up monitoring and reporting mechanisms to track the health and performance of the data ingestion pipeline.\n",
    "Monitor data quality metrics, capture statistics on data ingestion rates, and generate reports or dashboards for visibility.\n",
    "\n",
    "9. Scalability and performance: Design the pipeline to handle scalability and performance requirements. Consider distributed processing \n",
    "frameworks like Apache Spark or Hadoop to handle large volumes of data efficiently.\n",
    "\n",
    "10. Documentation and metadata management: Document the pipeline design, including the data sources, transformations, validation rules, \n",
    "and storage details. Maintain metadata about the ingested data, such as schema information, data lineage, and any data transformations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e147d27e-df7a-43e5-b012-270b44a9fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Model Training:\n",
    "   a. Build a machine learning model to predict customer churn based on a given dataset. Train the model using appropriate algorithms\n",
    "and evaluate its performance.\n",
    "   b. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, feature scaling, and \n",
    "    dimensionality reduction.\n",
    "   c. Train a deep learning model for image classification using transfer learning and fine-tuning techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389513c1-026a-463b-b3d8-a8bcd57843e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a. Building a Machine Learning Model for Customer Churn Prediction:\n",
    "\n",
    "To build a machine learning model for customer churn prediction, follow these steps:\n",
    "\n",
    "1. Data Preprocessing:\n",
    "   - Load the dataset and perform necessary data cleaning, such as handling missing values and removing irrelevant columns.\n",
    "   - Split the dataset into features (X) and the target variable (y), where y represents whether a customer churned or not.\n",
    "\n",
    "2. Feature Engineering:\n",
    "   - Perform feature engineering to extract relevant information from the dataset. This may include creating new features,\n",
    "transforming existing ones, or selecting important features.\n",
    "   - Apply techniques like one-hot encoding to convert categorical variables into numerical representations.\n",
    "   - Scale numerical features using techniques like standardization or normalization.\n",
    "\n",
    "3. Model Selection:\n",
    "   - Select an appropriate algorithm for customer churn prediction, such as logistic regression, decision trees, random forests, or\n",
    "gradient boosting algorithms like XGBoost or LightGBM.\n",
    "   - Consider the characteristics of your dataset and the interpretability requirements to choose the best algorithm.\n",
    "\n",
    "4. Model Training and Evaluation:\n",
    "   - Split the preprocessed data into training and testing sets.\n",
    "   - Train the chosen model on the training set.\n",
    "   - Evaluate the model's performance on the testing set using appropriate evaluation metrics such as accuracy, precision, recall, \n",
    "F1-score, or area under the ROC curve (AUC-ROC).\n",
    "   - Adjust the hyperparameters of the model to optimize its performance, using techniques like cross-validation or grid search.\n",
    "\n",
    "5. Model Deployment:\n",
    "   - Once satisfied with the model's performance, save the trained model for future use.\n",
    "   - Deploy the model in a production environment to make predictions on new customer data.\n",
    "\n",
    "b. Developing a Model Training Pipeline with Feature Engineering:\n",
    "\n",
    "To develop a model training pipeline that incorporates feature engineering techniques, such as one-hot encoding, feature scaling, and\n",
    "dimensionality reduction, follow these steps:\n",
    "\n",
    "1. Data Preprocessing:\n",
    "   - Load the dataset and perform necessary data cleaning.\n",
    "   - Split the dataset into features (X) and the target variable (y).\n",
    "\n",
    "2. Feature Engineering:\n",
    "   - Identify categorical features and apply one-hot encoding to convert them into numerical representations.\n",
    "   - Identify numerical features and apply feature scaling techniques such as standardization or normalization to normalize their scales.\n",
    "   - Optionally, apply dimensionality reduction techniques like Principal Component Analysis (PCA) or feature selection algorithms to reduce \n",
    "the number of features while retaining important information.\n",
    "\n",
    "3. Model Training and Evaluation:\n",
    "   - Split the preprocessed data into training and testing sets.\n",
    "   - Train the chosen machine learning model on the training set.\n",
    "   - Evaluate the model's performance on the testing set using appropriate evaluation metrics.\n",
    "   - Fine-tune the model and its hyperparameters based on the evaluation results.\n",
    "\n",
    "4. Model Deployment:\n",
    "   - Save the trained model and the preprocessing steps (such as one-hot encoding and feature scaling) as part of the pipeline.\n",
    "   - Deploy the pipeline in a production environment to preprocess new data and make predictions using the trained model.\n",
    "\n",
    "c. Training a Deep Learning Model for Image Classification using Transfer Learning and Fine-tuning:\n",
    "\n",
    "To train a deep learning model for image classification using transfer learning and fine-tuning techniques, follow these steps:\n",
    "\n",
    "1. Data Preparation:\n",
    "   - Obtain a labeled dataset of images for training, validation, and testing.\n",
    "   - Split the dataset into training, validation, and testing sets.\n",
    "\n",
    "2. Transfer Learning:\n",
    "   - Choose a pre-trained deep learning model such as VGG16, ResNet, or Inception.\n",
    "   - Load the pre-trained model weights without the classification layers.\n",
    "   - Freeze the weights of the pre-trained layers to retain their learned features.\n",
    "\n",
    "3. Model Architecture:\n",
    "   - Add new layers on top of the pre-trained model to adapt it to the image classification task.\n",
    "   - The new layers typically include a combination of convolutional, pooling, and fully connected layers.\n",
    "   - The last layer should have the same number of units as the number of classes in the classification task.\n",
    "\n",
    "4. Model Training:\n",
    "   - Compile the model with an appropriate loss function and optimizer.\n",
    "   - Train the model using the training set.\n",
    "   - Monitor the model's performance on the validation set and adjust hyperparameters if necessary.\n",
    "\n",
    "5. Fine-tuning:\n",
    "   - Unfreeze some of the earlier layers of the pre-trained model to allow them to be trained with the new dataset.\n",
    "   - Retrain the model using the training set, including both the new layers and the unfrozen pre-trained layers.\n",
    "   - Fine-tune the hyperparameters as needed.\n",
    "\n",
    "6. Model Evaluation:\n",
    "   - Evaluate the trained model's performance on the testing set using appropriate evaluation metrics such as accuracy, precision,\n",
    "recall, or F1-score.\n",
    "\n",
    "7. Model Deployment:\n",
    "   - Save the trained model for future use.\n",
    "   - Deploy the model in a production environment to classify new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36bfc23-2d88-49ff-9d88-897266cff4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Model Validation:\n",
    "   a. Implement cross-validation to evaluate the performance of a regression model for predicting housing prices.\n",
    "   b. Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary\n",
    "    classification problem.\n",
    "   c. Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05e4a3-ec4c-4db2-a5f2-fd70ced10ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a. Implementing cross-validation for regression model evaluation:\n",
    "\n",
    "Cross-validation is a technique used to assess the performance of a machine learning model on a dataset. \n",
    "It helps to obtain a more reliable estimate of the model's performance by splitting the data into multiple subsets and evaluating the\n",
    "model on each subset.\n",
    "\n",
    "Here's how you can implement cross-validation for evaluating the performance of a regression model for predicting housing prices:\n",
    "\n",
    "1. Split the data: Divide your dataset into K equal-sized subsets or \"folds\". The common choice for K is 5 or 10, but you can choose a \n",
    "different value based on your requirements.\n",
    "\n",
    "2. Train and evaluate: For each fold:\n",
    "   a. Use K-1 folds as training data and train your regression model.\n",
    "   b. Use the remaining fold as the validation data and evaluate the model's performance.\n",
    "\n",
    "3. Calculate performance metrics: After evaluating the model on each fold, calculate the performance metrics of interest, such as mean squared\n",
    "error (MSE), root mean squared error (RMSE), mean absolute error (MAE), or R-squared.\n",
    "\n",
    "4. Aggregate results: Finally, calculate the average performance across all the folds to get an overall estimate of the model's performance.\n",
    "\n",
    "b. Model validation using different evaluation metrics for binary classification:\n",
    "\n",
    "When dealing with binary classification problems, there are several evaluation metrics that can be used to validate the model's performance. \n",
    "Some commonly used metrics are accuracy, precision, recall, and F1 score.\n",
    "\n",
    "- Accuracy: The accuracy metric measures the overall correctness of the model's predictions, calculated as the ratio of correctly predicted \n",
    "instances to the total number of instances.\n",
    "\n",
    "- Precision: Precision represents the proportion of correctly predicted positive instances out of all predicted positive instances. \n",
    "It measures the model's ability to avoid false positives.\n",
    "\n",
    "- Recall: Recall, also known as sensitivity or true positive rate, measures the proportion of correctly predicted positive instances out\n",
    "of all actual positive instances. It quantifies the model's ability to identify positive instances.\n",
    "\n",
    "- F1 score: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that considers both precision and recall.\n",
    "\n",
    "To perform model validation, follow these steps:\n",
    "\n",
    "1. Split the data: Split your dataset into training and testing sets. The common split ratio is 80:20 or 70:30, where the larger portion is \n",
    "used for training.\n",
    "\n",
    "2. Train the model: Use the training data to train your binary classification model.\n",
    "\n",
    "3. Make predictions: Use the trained model to make predictions on the testing data.\n",
    "\n",
    "4. Calculate evaluation metrics: Calculate accuracy, precision, recall, and F1 score using the predicted labels and the true labels from the \n",
    "testing data.\n",
    "\n",
    "5. Interpret results: Analyze the evaluation metrics to assess the model's performance. Different metrics highlight different aspects of the model's\n",
    "behavior, so it's important to consider them collectively.\n",
    "\n",
    "c. Model validation strategy incorporating stratified sampling for imbalanced datasets:\n",
    "\n",
    "When dealing with imbalanced datasets, where one class is significantly more prevalent than the other, standard model validation techniques can be \n",
    "problematic. Stratified sampling is a technique that helps overcome this issue by ensuring that the proportions of the classes are maintained in \n",
    "both the training and testing datasets.\n",
    "\n",
    "Here's how you can design a model validation strategy that incorporates stratified sampling for imbalanced datasets:\n",
    "\n",
    "1. Split the data: Divide your imbalanced dataset into training and testing sets, while maintaining the class proportions.\n",
    "\n",
    "2. Stratified sampling: Use stratified sampling to split the data in a way that ensures the distribution of the minority class is preserved in \n",
    "both the training and testing sets. This can be achieved using functions or libraries specifically designed for stratified sampling, which consider \n",
    "the class labels during the split.\n",
    "\n",
    "3. Train the model: Use the training data to train your model.\n",
    "\n",
    "4. Evaluate the model: Use the testing data to evaluate the model's performance, calculating evaluation metrics such as accuracy, precision, recall,\n",
    "and F1 score.\n",
    "\n",
    "By incorporating stratified sampling, you ensure that the model is trained and evaluated on data that is representative of the real-world class\n",
    "distribution. This helps prevent biased results and provides a more accurate assessment of the model's performance on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef86be-7716-42de-a1c6-7942caccf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Deployment Strategy:\n",
    "   a. Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions.\n",
    "   b. Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure.\n",
    "   c. Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce131710-d634-42c0-ad77-3412ee5eda2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a. Deployment Strategy for Real-Time Recommendations:\n",
    "1. Preprocessing and Feature Engineering: Prepare the input data by preprocessing and performing feature engineering\n",
    "to extract relevant features that the model requires for making recommendations.\n",
    "\n",
    "2. Model Selection and Training: Choose an appropriate machine learning model, such as collaborative filtering, \n",
    "content-based filtering, or hybrid models, based on the nature of the recommendation task. Train the selected model using \n",
    "historical user interaction data and optimize its parameters.\n",
    "\n",
    "3. Real-Time Inference System: Set up a scalable and low-latency infrastructure to handle real-time user requests. This can\n",
    "be achieved by deploying the model on a cloud platform or using serverless computing. Ensure that the infrastructure can handle \n",
    "high concurrent request loads and automatically scale up or down based on demand.\n",
    "\n",
    "4. API Development: Develop an API endpoint that accepts user input and sends it to the deployed model for inference. The API\n",
    "should handle authentication, input validation, and error handling. It should also return the recommended items or actions to the user in real-time.\n",
    "\n",
    "5. Integration with User Interface: Integrate the recommendation system API with the user interface, such as a website or mobile \n",
    "application, to display the real-time recommendations to users. Ensure seamless integration and a smooth user experience.\n",
    "\n",
    "6. A/B Testing and Evaluation: Implement A/B testing to compare the performance of different recommendation algorithms or variations\n",
    "Continuously monitor and evaluate the recommendations' quality and user engagement metrics to improve the model and its performance over time.\n",
    "\n",
    "b. Deployment Pipeline for Machine Learning Models:\n",
    "1. Version Control: Use a version control system, such as Git, to manage the source code and model artifacts. Keep track of changes\n",
    "and maintain a history of model versions.\n",
    "\n",
    "2. Continuous Integration and Testing: Set up a continuous integration (CI) system to automatically build and test the model code.\n",
    "Run unit tests, integration tests, and other relevant checks to ensure the codebase is in a deployable state.\n",
    "\n",
    "3. Packaging and Containerization: Package the model code, dependencies, and any required configurations into a container, such as \n",
    "Docker. This ensures consistency across different environments and facilitates easy deployment.\n",
    "\n",
    "4. Infrastructure Orchestration: Use infrastructure-as-code tools, such as AWS CloudFormation or Azure Resource Manager, to define\n",
    "and provision the necessary cloud resources for deploying the model, such as virtual machines, storage, and networking components.\n",
    "\n",
    "5. Automated Deployment: Use a deployment orchestration tool, like Jenkins or AWS CodePipeline, to automate the deployment process.\n",
    "Set up deployment pipelines that trigger on code changes or new model versions and deploy the packaged model to the target cloud platform.\n",
    "\n",
    "6. Integration Testing: Perform integration tests on the deployed model to ensure it functions correctly within the target environment.\n",
    "Test the API endpoints, data connectivity, and any other dependencies.\n",
    "\n",
    "7. Continuous Monitoring: Implement monitoring solutions to collect metrics and logs from the deployed model, including performance,\n",
    "resource utilization, error rates, and response times. Use tools like AWS CloudWatch or Azure Monitor to gain insights into the model's behavior.\n",
    "\n",
    "c. Monitoring and Maintenance Strategy for Deployed Models:\n",
    "1. Performance Monitoring: Continuously monitor the model's performance metrics, such as accuracy, precision, recall, and latency. \n",
    "Set up alerts to notify the team when performance falls below acceptable thresholds.\n",
    "\n",
    "\n",
    "2. Error Monitoring and Logging: Implement comprehensive error monitoring and logging mechanisms to capture and analyze errors or\n",
    "exceptions generated during the model's execution. This helps identify and diagnose issues quickly.\n",
    "\n",
    "3. Data Drift Detection: Monitor the distribution and quality of incoming data to detect any shifts or drifts that might affect the model's\n",
    "performance. Implement statistical tests or anomaly detection techniques to identify and handle data drift.\n",
    "\n",
    "4. Regular Retraining: Periodically retrain the deployed model using the latest available data. Determine an appropriate retraining \n",
    "frequency based on the rate of data change and the model's performance degradation over time.\n",
    "\n",
    "5. Model Versioning: Maintain a clear versioning system for the deployed models. This allows for easy rollback to previous versions if\n",
    "necessary and enables comparison of different models' performance.\n",
    "\n",
    "6. Security and Privacy: Implement security measures to protect the model and user data. Use encryption, access controls, and authentication\n",
    "mechanisms to ensure data privacy and prevent unauthorized access to the deployed system.\n",
    "\n",
    "7. Feedback Loop and Continuous Improvement: Gather feedback from users and stakeholders to identify areas for improvement. Incorporate user \n",
    "feedback into the model's training process and iterate on the deployment pipeline to enhance the system's performance and reliability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
